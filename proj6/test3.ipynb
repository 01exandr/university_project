{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "# from PIL import Image\n",
    "\n",
    "# # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Faster R-CNN –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "# model.eval()\n",
    "\n",
    "# # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è CPU –∞–±–æ CUDA (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "# def read_image(image_path):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     return np.array(image)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –æ–±'—î–∫—Ç—ñ–≤\n",
    "# def get_prediction(image_path, model, confidence_threshold=0.5):\n",
    "#     image = read_image(image_path)\n",
    "#     image_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "#     # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å\n",
    "#     with torch.no_grad():\n",
    "#         predictions = model(image_tensor)[0]\n",
    "\n",
    "#     # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –∑–∞ –ø–æ—Ä–æ–≥–æ–º –¥–æ–≤—ñ—Ä–∏\n",
    "#     boxes = predictions['boxes'].cpu().numpy()\n",
    "#     scores = predictions['scores'].cpu().numpy()\n",
    "#     labels = predictions['labels'].cpu().numpy()\n",
    "\n",
    "#     selected_indices = np.where(scores >= confidence_threshold)[0]\n",
    "#     selected_boxes = boxes[selected_indices]\n",
    "#     selected_labels = labels[selected_indices]\n",
    "#     selected_scores = scores[selected_indices]\n",
    "\n",
    "#     return selected_boxes, selected_labels, selected_scores\n",
    "\n",
    "# # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "# def visualize_predictions(image_path, boxes, labels, scores):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     for box, label, score in zip(boxes, labels, scores):\n",
    "#         x_min, y_min, x_max, y_max = map(int, box)\n",
    "#         cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "#         label_text = f\"Label: {label}, Score: {score:.2f}\"\n",
    "#         cv2.putText(image, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "#     # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∞–±–æ –ø–æ–∫–∞–∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è–º–∏\n",
    "#     output_path = \"prediction_visual1.png\"\n",
    "#     cv2.imwrite(output_path, image)\n",
    "#     # cv2.imshow(\"predict\", image)\n",
    "#     # cv2.waitKey(0)\n",
    "#     # cv2.destroyAllWindows()\n",
    "#     # Image(image)\n",
    "\n",
    "# # –û—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å\n",
    "# def main():\n",
    "#     image_path = \"image1.jpg\"  # –ó–∞–º—ñ–Ω–∏ –Ω–∞ —Å–≤—ñ–π —à–ª—è—Ö –¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "#     boxes, labels, scores = get_prediction(image_path, model)\n",
    "#     visualize_predictions(image_path, boxes, labels, scores)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "# from IPython.display import Image\n",
    "# from PIL import Image as PILImage\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# os.getcwd()\n",
    "\n",
    "# # Define AutoDetectionModel class\n",
    "# class AutoDetectionModel:\n",
    "#     def __init__(self, model, confidence_threshold, image_size, device, load_at_init):\n",
    "#         self.model = model\n",
    "#         self.confidence_threshold = confidence_threshold\n",
    "#         self.image_size = image_size\n",
    "#         self.device = device\n",
    "#         self.load_at_init = load_at_init\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, model_type, model, confidence_threshold, image_size, device, load_at_init=True):\n",
    "#         return cls(model, confidence_threshold, image_size, device, load_at_init)\n",
    "\n",
    "#     def predict(self, image):\n",
    "#         with torch.no_grad():\n",
    "#             image = [img.to(self.device) for img in image]\n",
    "#             predictions = self.model(image)\n",
    "#             return predictions\n",
    "\n",
    "# # Define get_sliced_prediction function\n",
    "# def get_sliced_prediction(image_path, detection_model, slice_height, slice_width, overlap_height_ratio, overlap_width_ratio):\n",
    "#     image = PILImage.open(image_path).convert(\"RGB\")\n",
    "#     image_np = np.array(image)\n",
    "#     height, width, _ = image_np.shape\n",
    "\n",
    "#     slice_height_overlap = int(slice_height * overlap_height_ratio)\n",
    "#     slice_width_overlap = int(slice_width * overlap_width_ratio)\n",
    "\n",
    "#     results = []\n",
    "#     for y in range(0, height, slice_height - slice_height_overlap):\n",
    "#         for x in range(0, width, slice_width - slice_width_overlap):\n",
    "#             slice_x1 = x\n",
    "#             slice_y1 = y\n",
    "#             slice_x2 = min(x + slice_width, width)\n",
    "#             slice_y2 = min(y + slice_height, height)\n",
    "\n",
    "#             slice_image = image_np[slice_y1:slice_y2, slice_x1:slice_x2]\n",
    "#             slice_image_tensor = torchvision.transforms.functional.to_tensor(slice_image)\n",
    "\n",
    "#             predictions = detection_model.predict([slice_image_tensor])[0]\n",
    "#             filtered_boxes = []\n",
    "#             filtered_scores = []\n",
    "#             for box, score, label in zip(predictions['boxes'], predictions['scores'], predictions['labels']):\n",
    "#                 if score >= detection_model.confidence_threshold:\n",
    "#                     filtered_boxes.append(box)\n",
    "#                     filtered_scores.append(score)\n",
    "#             predictions['boxes'] = filtered_boxes\n",
    "#             predictions['scores'] = filtered_scores\n",
    "#             results.append((predictions, (slice_x1, slice_y1, slice_x2, slice_y2)))\n",
    "\n",
    "#     return SlicedPrediction(image_np, results)\n",
    "\n",
    "# # Define SlicedPrediction class\n",
    "# class SlicedPrediction:\n",
    "#     def __init__(self, original_image, results):\n",
    "#         self.original_image = original_image\n",
    "#         self.results = results\n",
    "\n",
    "#     def export_visuals(self, export_dir=\"\"):\n",
    "#         if not os.path.exists(export_dir) and export_dir != \"\":\n",
    "#             os.makedirs(export_dir)\n",
    "#         fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "#         ax.imshow(self.original_image)\n",
    "#         for predictions, (x1, y1, x2, y2) in self.results:\n",
    "#             for box, score in zip(predictions['boxes'], predictions['scores']):\n",
    "#                 if score >= 0.7:  # Adjust confidence threshold for visualization\n",
    "#                     rect = patches.Rectangle((x1 + box[0], y1 + box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='red', facecolor='none')\n",
    "#                     ax.add_patch(rect)\n",
    "#         plt.axis('off')\n",
    "#         output_path = f\"{export_dir}/prediction_visual.png\"\n",
    "#         plt.savefig(output_path)\n",
    "#         plt.close(fig)\n",
    "#         return output_path\n",
    "\n",
    "# # set torchvision FasterRCNN model\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# # initialize detection model using AutoDetectionModel\n",
    "# detection_model = AutoDetectionModel.from_pretrained(\n",
    "#     model_type='torchvision',\n",
    "#     model=model,\n",
    "#     confidence_threshold=0.5,\n",
    "#     image_size=640,\n",
    "#     device=\"cpu\", # or \"cuda:0\"\n",
    "#     load_at_init=True,\n",
    "# )\n",
    "\n",
    "# # perform prediction using sliced inference\n",
    "# result = get_sliced_prediction(\n",
    "#     \"image1.jpg\",\n",
    "#     detection_model,\n",
    "#     slice_height=320,\n",
    "#     slice_width=320,\n",
    "#     overlap_height_ratio=0.2,\n",
    "#     overlap_width_ratio=0.2,\n",
    "# )\n",
    "\n",
    "# # export the prediction visuals\n",
    "# output_path = result.export_visuals(export_dir=\"\")\n",
    "\n",
    "# # display the result image\n",
    "# Image(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"using\", device, \"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sahi.utils.yolov5 import download_yolov5s6_model\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import torch\n",
    "import os\n",
    "# from google.colab.patches import cv2_imshow\n",
    "# Install the required packages\n",
    "# try:\n",
    "#     import yolov5\n",
    "# except ModuleNotFoundError:\n",
    "#     print(\"YOLOv5 is not installed. Installing now...\")\n",
    "#     !pip install -U torch sahi yolov5\n",
    "\n",
    "# Download YOLOv5 model\n",
    "model_path = 'models/yolov5s6.pt'\n",
    "download_yolov5s6_model(destination_path=model_path)\n",
    "\n",
    "# Initialize the detection model\n",
    "confidence_threshold = 0.3\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Load the video file\n",
    "video_path = \"/content/Cars Moving On Road Stock Footage - Free Download.mp4\"  # Replace with the path to your video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video was successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Exit the loop if there are no frames left to read\n",
    "\n",
    "    # Get sliced prediction for the current frame\n",
    "    prediction = get_sliced_prediction(frame, detection_model, slice_height=256, slice_width=256, overlap_height_ratio=0.2, overlap_width_ratio=0.2)\n",
    "\n",
    "    # Draw the detections on the frame\n",
    "    for obj in prediction.object_prediction_list:\n",
    "        if obj.category.name == \"car\":  # Filter for cars\n",
    "            bbox = obj.bbox.to_xyxy()\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"{obj.category.name}: {obj.score:.2f}\" if isinstance(obj.score, float) else f\"{obj.category.name}: {obj.score}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with detections\n",
    "    cv2.imshow(frame)\n",
    "\n",
    "    # Press 'q' to exit the video early\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_slice_bboxes(\n",
    "    image_height: int,\n",
    "    image_width: int,\n",
    "    slice_height: Optional[int] = None,\n",
    "    slice_width: Optional[int] = None,\n",
    "    auto_slice_resolution: bool = True,\n",
    "    overlap_height_ratio: float = 0.2,\n",
    "    overlap_width_ratio: float = 0.2,\n",
    ") -> List[List[int]]:\n",
    "\n",
    "    slice_bboxes = []\n",
    "    y_max = y_min = 0\n",
    "\n",
    "    if slice_height and slice_width:\n",
    "        y_overlap = int(overlap_height_ratio * slice_height)\n",
    "        x_overlap = int(overlap_width_ratio * slice_width)\n",
    "    else:\n",
    "        raise ValueError(\"Compute type is not auto and slice width and height are not provided.\")\n",
    "\n",
    "    while y_max < image_height:\n",
    "        x_min = x_max = 0\n",
    "        y_max = y_min + slice_height\n",
    "        while x_max < image_width:\n",
    "            x_max = x_min + slice_width\n",
    "            if y_max > image_height or x_max > image_width:\n",
    "                xmax = min(image_width, x_max)\n",
    "                ymax = min(image_height, y_max)\n",
    "                xmin = max(0, xmax - slice_width)\n",
    "                ymin = max(0, ymax - slice_height)\n",
    "                slice_bboxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                slice_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "            x_min = x_max - x_overlap\n",
    "        y_min = y_max - y_overlap\n",
    "    return slice_bboxes\n",
    "\n",
    "\n",
    "class SlicedImage:\n",
    "    def __init__(self, image, starting_pixel):\n",
    "        self.image = image\n",
    "        self.starting_pixel = starting_pixel\n",
    "\n",
    "\n",
    "class SliceImageResult:\n",
    "    def __init__(self, original_image_size: List[int], image_dir: Optional[str] = None):\n",
    "        self.original_image_height = original_image_size[0]\n",
    "        self.original_image_width = original_image_size[1]\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        self._sliced_image_list: List[SlicedImage] = []\n",
    "\n",
    "    def add_sliced_image(self, sliced_image: SlicedImage):\n",
    "        if not isinstance(sliced_image, SlicedImage):\n",
    "            raise TypeError(\"sliced_image must be a SlicedImage instance\")\n",
    "\n",
    "        self._sliced_image_list.append(sliced_image)\n",
    "\n",
    "    @property\n",
    "    def sliced_image_list(self):\n",
    "        return self._sliced_image_list\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        images = []\n",
    "        for sliced_image in self._sliced_image_list:\n",
    "            images.append(sliced_image.image)\n",
    "        return images\n",
    "\n",
    "    @property\n",
    "    def starting_pixels(self) -> List[int]:\n",
    "        starting_pixels = []\n",
    "        for sliced_image in self._sliced_image_list:\n",
    "            starting_pixels.append(sliced_image.starting_pixel)\n",
    "        return starting_pixels\n",
    "\n",
    "    @property\n",
    "    def filenames(self) -> List[int]:\n",
    "        filenames = []\n",
    "        for sliced_image in self._sliced_image_list:\n",
    "            filenames.append(sliced_image.coco_image.file_name)\n",
    "        return filenames\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        def _prepare_ith_dict(i):\n",
    "            return {\n",
    "                \"image\": self.images[i],\n",
    "                \"starting_pixel\": self.starting_pixels[i],\n",
    "            }\n",
    "\n",
    "        if isinstance(i, np.ndarray):\n",
    "            i = i.tolist()\n",
    "\n",
    "        if isinstance(i, int):\n",
    "            return _prepare_ith_dict(i)\n",
    "        elif isinstance(i, slice):\n",
    "            start, stop, step = i.indices(len(self))\n",
    "            return [_prepare_ith_dict(i) for i in range(start, stop, step)]\n",
    "        elif isinstance(i, (tuple, list)):\n",
    "            accessed_mapping = map(_prepare_ith_dict, i)\n",
    "            return list(accessed_mapping)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{type(i)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._sliced_image_list)\n",
    "\n",
    "\n",
    "def slice_image(\n",
    "    image: Union[str, Image.Image],\n",
    "    slice_height: Optional[int] = None,\n",
    "    slice_width: Optional[int] = None,\n",
    "    overlap_height_ratio: float = 0.2,\n",
    "    overlap_width_ratio: float = 0.2,\n",
    "    auto_slice_resolution: bool = True,\n",
    "    min_area_ratio: float = 0.1,\n",
    "    out_ext: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> SliceImageResult:\n",
    "\n",
    "    image_pil = image\n",
    "\n",
    "    image_width, image_height = image_pil.size\n",
    "    if not (image_width != 0 and image_height != 0):\n",
    "        raise RuntimeError(f\"invalid image size: {image_pil.size} for 'slice_image'.\")\n",
    "    slice_bboxes = get_slice_bboxes(\n",
    "        image_height=image_height,\n",
    "        image_width=image_width,\n",
    "        auto_slice_resolution=auto_slice_resolution,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio,\n",
    "    )\n",
    "\n",
    "    n_ims = 0\n",
    "\n",
    "    sliced_image_result = SliceImageResult(original_image_size=[image_height, image_width])\n",
    "\n",
    "    image_pil_arr = np.asarray(image_pil)\n",
    "\n",
    "    for slice_bbox in slice_bboxes:\n",
    "        n_ims += 1\n",
    "\n",
    "        tlx = slice_bbox[0]\n",
    "        tly = slice_bbox[1]\n",
    "        brx = slice_bbox[2]\n",
    "        bry = slice_bbox[3]\n",
    "        image_pil_slice = image_pil_arr[tly:bry, tlx:brx]\n",
    "\n",
    "        slice_width = slice_bbox[2] - slice_bbox[0]\n",
    "        slice_height = slice_bbox[3] - slice_bbox[1]\n",
    "\n",
    "        sliced_image = SlicedImage(\n",
    "            image=image_pil_slice, starting_pixel=[slice_bbox[0], slice_bbox[1]]\n",
    "        )\n",
    "        sliced_image_result.add_sliced_image(sliced_image)\n",
    "\n",
    "    return sliced_image_result\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"D:\\magl\\code_jet\\Cars1.mp4\") #\"road.mp4\" \"Cars1.mp4\"\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "\n",
    "# Get the width and height of the video frame\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    image = Image.fromarray(frame)\n",
    "    height, width = image.size\n",
    "\n",
    "    slice_height = int(height/4)\n",
    "    slice_width = int(width/4)\n",
    "\n",
    "    slice_image_result = slice_image(\n",
    "            image=image,\n",
    "            slice_height=slice_height,\n",
    "            slice_width=slice_width,\n",
    "            overlap_height_ratio=0.2,\n",
    "            overlap_width_ratio=0.2,\n",
    "            auto_slice_resolution=False,\n",
    "        )\n",
    "\n",
    "    number_of_tiles = len(slice_image_result)\n",
    "\n",
    "    results = []\n",
    "    bboxes = []\n",
    "    confs = []\n",
    "    class_ids = []\n",
    "\n",
    "    for i, image_slice in enumerate(slice_image_result):\n",
    "\n",
    "        window = image_slice['image']\n",
    "        start_x, start_y = image_slice['starting_pixel']\n",
    "\n",
    "        results = model.predict(window, conf=0.2)\n",
    "\n",
    "        for result in results:\n",
    "\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "            if xyxy.size == 0:\n",
    "                continue\n",
    "\n",
    "            xyxy = xyxy\n",
    "            conf = boxes.conf.cpu().numpy()\n",
    "            class_id = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for i in range(len(xyxy)):\n",
    "\n",
    "                x1, y1, x2, y2 = xyxy[i]\n",
    "\n",
    "                x1 += start_x\n",
    "                y1 += start_y\n",
    "                x2 += start_x\n",
    "                y2 += start_y\n",
    "\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                bboxes.append([x1, y1, x2, y2])\n",
    "                confs.append(conf[i])\n",
    "                class_ids.append(class_id[i])\n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    fps = 1 / total_time\n",
    "\n",
    "    for i in range(len(bboxes)):\n",
    "        x1, y1, x2, y2 = bboxes[i]\n",
    "\n",
    "        conf = confs[i]\n",
    "        class_id = class_ids[i]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{model.names[int(class_id)]} {conf:.1f}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{number_of_tiles}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{fps:.1f}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Write the frame into the file 'output.mp4'\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # cv2.imshow('frame', frame)\n",
    "    # #cv2_imshow(frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def get_slice_bboxes(\n",
    "    image_height: int,\n",
    "    image_width: int,\n",
    "    slice_height: Optional[int] = None,\n",
    "    slice_width: Optional[int] = None,\n",
    "    auto_slice_resolution: bool = True,\n",
    "    overlap_height_ratio: float = 0.2,\n",
    "    overlap_width_ratio: float = 0.2,\n",
    ") -> List[List[int]]:\n",
    "\n",
    "    slice_bboxes = []\n",
    "    y_max = y_min = 0\n",
    "\n",
    "    if slice_height and slice_width:\n",
    "        y_overlap = int(overlap_height_ratio * slice_height)\n",
    "        x_overlap = int(overlap_width_ratio * slice_width)\n",
    "    else:\n",
    "        raise ValueError(\"Compute type is not auto and slice width and height are not provided.\")\n",
    "\n",
    "    while y_max < image_height:\n",
    "        x_min = x_max = 0\n",
    "        y_max = y_min + slice_height\n",
    "        while x_max < image_width:\n",
    "            x_max = x_min + slice_width\n",
    "            if y_max > image_height or x_max > image_width:\n",
    "                xmax = min(image_width, x_max)\n",
    "                ymax = min(image_height, y_max)\n",
    "                xmin = max(0, xmax - slice_width)\n",
    "                ymin = max(0, ymax - slice_height)\n",
    "                slice_bboxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                slice_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "            x_min = x_max - x_overlap\n",
    "        y_min = y_max - y_overlap\n",
    "    return slice_bboxes\n",
    "\n",
    "\n",
    "class SlicedImage:\n",
    "    def __init__(self, image, starting_pixel):\n",
    "        self.image = image\n",
    "        self.starting_pixel = starting_pixel\n",
    "\n",
    "\n",
    "class SliceImageResult:\n",
    "    def __init__(self, original_image_size: List[int], image_dir: Optional[str] = None):\n",
    "        self.original_image_height = original_image_size[0]\n",
    "        self.original_image_width = original_image_size[1]\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        self._sliced_image_list: List[SlicedImage] = []\n",
    "\n",
    "    def add_sliced_image(self, sliced_image: SlicedImage):\n",
    "        if not isinstance(sliced_image, SlicedImage):\n",
    "            raise TypeError(\"sliced_image must be a SlicedImage instance\")\n",
    "\n",
    "        self._sliced_image_list.append(sliced_image)\n",
    "\n",
    "    @property\n",
    "    def sliced_image_list(self):\n",
    "        return self._sliced_image_list\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        images = []\n",
    "        for sliced_image in self._sliced_image_list:\n",
    "            images.append(sliced_image.image)\n",
    "        return images\n",
    "\n",
    "    @property\n",
    "    def starting_pixels(self) -> List[int]:\n",
    "        starting_pixels = []\n",
    "        for sliced_image in self._sliced_image_list:\n",
    "            starting_pixels.append(sliced_image.starting_pixel)\n",
    "        return starting_pixels\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        def _prepare_ith_dict(i):\n",
    "            return {\n",
    "                \"image\": self.images[i],\n",
    "                \"starting_pixel\": self.starting_pixels[i],\n",
    "            }\n",
    "\n",
    "        if isinstance(i, np.ndarray):\n",
    "            i = i.tolist()\n",
    "\n",
    "        if isinstance(i, int):\n",
    "            return _prepare_ith_dict(i)\n",
    "        elif isinstance(i, slice):\n",
    "            start, stop, step = i.indices(len(self))\n",
    "            return [_prepare_ith_dict(i) for i in range(start, stop, step)]\n",
    "        elif isinstance(i, (tuple, list)):\n",
    "            accessed_mapping = map(_prepare_ith_dict, i)\n",
    "            return list(accessed_mapping)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{type(i)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._sliced_image_list)\n",
    "\n",
    "\n",
    "def slice_image(\n",
    "    image: Union[str, Image.Image],\n",
    "    slice_height: Optional[int] = None,\n",
    "    slice_width: Optional[int] = None,\n",
    "    overlap_height_ratio: float = 0.2,\n",
    "    overlap_width_ratio: float = 0.2,\n",
    "    auto_slice_resolution: bool = True,\n",
    "    min_area_ratio: float = 0.1,\n",
    "    out_ext: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> SliceImageResult:\n",
    "\n",
    "    image_pil = image\n",
    "\n",
    "    image_width, image_height = image_pil.size\n",
    "    if not (image_width != 0 and image_height != 0):\n",
    "        raise RuntimeError(f\"invalid image size: {image_pil.size} for 'slice_image'.\")\n",
    "    slice_bboxes = get_slice_bboxes(\n",
    "        image_height=image_height,\n",
    "        image_width=image_width,\n",
    "        auto_slice_resolution=auto_slice_resolution,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio,\n",
    "    )\n",
    "\n",
    "    n_ims = 0\n",
    "\n",
    "    sliced_image_result = SliceImageResult(original_image_size=[image_height, image_width])\n",
    "\n",
    "    image_pil_arr = np.asarray(image_pil)\n",
    "\n",
    "    for slice_bbox in slice_bboxes:\n",
    "        n_ims += 1\n",
    "\n",
    "        tlx = slice_bbox[0]\n",
    "        tly = slice_bbox[1]\n",
    "        brx = slice_bbox[2]\n",
    "        bry = slice_bbox[3]\n",
    "        image_pil_slice = image_pil_arr[tly:bry, tlx:brx]\n",
    "\n",
    "        sliced_image = SlicedImage(\n",
    "            image=image_pil_slice, starting_pixel=[slice_bbox[0], slice_bbox[1]]\n",
    "        )\n",
    "        sliced_image_result.add_sliced_image(sliced_image)\n",
    "\n",
    "    return sliced_image_result\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"D:\\magl\\code_jet\\Cars1.mp4\")\n",
    "\n",
    "model = YOLO(\"D:\\magl\\code_jet\\last.pt\")\n",
    "\n",
    "# Check if CUDA is available and set the device to GPU if possible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Get the width and height of the video frame\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "out = cv2.VideoWriter('output2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
    "\n",
    "# Define transformation to convert image slice to the required BCHW format\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((640, 640)),  # Resize to model input size\n",
    "])\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Remove CUDA GpuMat usage since OpenCV might not have CUDA support\n",
    "    # Directly use the frame for processing\n",
    "    image = Image.fromarray(frame)\n",
    "    height, width = image.size\n",
    "\n",
    "    slice_height = int(height / 1)\n",
    "    slice_width = int(width / 1)\n",
    "\n",
    "    slice_image_result = slice_image(\n",
    "        image=image,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=0.9, #0.2\n",
    "        overlap_width_ratio=0.9,\n",
    "        auto_slice_resolution=False,\n",
    "    )\n",
    "\n",
    "    number_of_tiles = len(slice_image_result)\n",
    "\n",
    "    bboxes = []\n",
    "    confs = []\n",
    "    class_ids = []\n",
    "\n",
    "    for i, image_slice in enumerate(slice_image_result):\n",
    "        window = image_slice['image']\n",
    "        start_x, start_y = image_slice['starting_pixel']\n",
    "\n",
    "        # Convert the window to tensor and resize to match model input requirements\n",
    "        window_tensor = transform(Image.fromarray(window)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        results = model.predict(window_tensor, conf=0.2, device=device)\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "            if xyxy.size == 0:\n",
    "                continue\n",
    "\n",
    "            conf = boxes.conf.cpu().numpy()\n",
    "            class_id = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for i in range(len(xyxy)):\n",
    "                x1, y1, x2, y2 = xyxy[i]\n",
    "\n",
    "                x1 += start_x\n",
    "                y1 += start_y\n",
    "                x2 += start_x\n",
    "                y2 += start_y\n",
    "\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                bboxes.append([x1, y1, x2, y2])\n",
    "                confs.append(conf[i])\n",
    "                class_ids.append(class_id[i])\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    fps = 1 / total_time\n",
    "\n",
    "    for i in range(len(bboxes)):\n",
    "        x1, y1, x2, y2 = bboxes[i]\n",
    "\n",
    "        conf = confs[i]\n",
    "        class_id = class_ids[i]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{model.names[int(class_id)]} {conf:.1f}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{number_of_tiles}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{fps:.1f}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame into the file 'output.mp4'\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.detect.predict import DetectionPredictor\n",
    "import cv2\n",
    "\n",
    "model = YOLO(\"D:\\magl\\code_jet\\yolov8s.pt\")\n",
    "\n",
    "results = model.predict(source=\"D:\\magl\\code_jet\\IMG_9014.MP4\", show=True)\n",
    "\n",
    "#print(result)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO üöÄ, AGPL-3.0 license\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.yolov8 import download_yolov8s_model\n",
    "\n",
    "from ultralytics.utils.files import increment_path\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "\n",
    "class SAHIInference:\n",
    "    \"\"\"Runs YOLOv8 and SAHI for object detection on video with options to view, save, and track results.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the SAHIInference class for performing sliced inference using SAHI with YOLOv8 models.\"\"\"\n",
    "        self.detection_model = None\n",
    "\n",
    "    def load_model(self, weights):\n",
    "        \"\"\"Loads a YOLOv8 model with specified weights for object detection using SAHI.\"\"\"\n",
    "        yolov8_model_path = f\"models/{weights}\"\n",
    "        download_yolov8s_model(yolov8_model_path)\n",
    "        self.detection_model = AutoDetectionModel.from_pretrained(\n",
    "            model_type=\"yolov8\", model_path=yolov8_model_path, confidence_threshold=0.3, device=\"cuda\"\n",
    "        )\n",
    "\n",
    "    def inference(\n",
    "        self, weights=\"yolov8n.pt\", source=\"test.mp4\", view_img=False, save_img=False, exist_ok=False, track=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run object detection on a video using YOLOv8 and SAHI.\n",
    "\n",
    "        Args:\n",
    "            weights (str): Model weights path.\n",
    "            source (str): Video file path.\n",
    "            view_img (bool): Show results.\n",
    "            save_img (bool): Save results.\n",
    "            exist_ok (bool): Overwrite existing files.\n",
    "            track (bool): Enable object tracking with SAHI\n",
    "        \"\"\"\n",
    "        # Video setup\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        assert cap.isOpened(), \"Error reading video file\"\n",
    "        frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "        # Output setup\n",
    "        save_dir = increment_path(Path(\"ultralytics_results_with_sahi\") / \"exp\", exist_ok)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        video_writer = cv2.VideoWriter(\n",
    "            str(save_dir / f\"{Path(source).stem}.mp4\"),\n",
    "            cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "            int(cap.get(5)),\n",
    "            (frame_width, frame_height),\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        self.load_model(weights)\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            annotator = Annotator(frame)  # Initialize annotator for plotting detection and tracking results\n",
    "            results = get_sliced_prediction(\n",
    "                frame,\n",
    "                self.detection_model,\n",
    "                slice_height=512,\n",
    "                slice_width=512,\n",
    "                overlap_height_ratio=0.2,\n",
    "                overlap_width_ratio=0.2,\n",
    "            )\n",
    "            detection_data = [\n",
    "                (det.category.name, det.category.id, (det.bbox.minx, det.bbox.miny, det.bbox.maxx, det.bbox.maxy))\n",
    "                for det in results.object_prediction_list\n",
    "            ]\n",
    "\n",
    "            for det in detection_data:\n",
    "                annotator.box_label(det[2], label=str(det[0]), color=colors(int(det[1]), True))\n",
    "\n",
    "            if view_img:\n",
    "                cv2.imshow(Path(source).stem, frame)\n",
    "            if save_img:\n",
    "                video_writer.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        video_writer.release()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def parse_opt(self):\n",
    "        \"\"\"Parse command line arguments.\"\"\"\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--weights\", type=str, default=\"yolov8n.pt\", help=\"initial weights path\")\n",
    "        parser.add_argument(\"--source\", type=str, required=True, help=\"video file path\")\n",
    "        parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n",
    "        parser.add_argument(\"--save-img\", action=\"store_true\", help=\"save results\")\n",
    "        parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n",
    "        return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inference = SAHIInference()\n",
    "    # inference.inference(**vars(inference.parse_opt()))\n",
    "    inference.inference(source=\"D:\\magl\\code_jet\\Cars1.mp4\", view_img=True, save_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "\n",
    "model = YOLO(\"D:\\magl\\code_jet\\last.pt\")  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format=\"onnx\")\n",
    "# engine or onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.export(format=\"engine\", batch=8, workspace=4, int8=True, data=\"coco.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è YOLOv8 –≤–∞–≥ —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–æ–≥–æ —Ñ–∞–π–ª—É\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the exported ONNX model\n",
    "onnx_model = YOLO(\"yolov8s.onnx\")\n",
    "\n",
    "# Run inference\n",
    "results = onnx_model(\"D:\\magl\\code_jet\\image1.jpg\")\n",
    "\n",
    "# –û–±—Ä–æ–±–∫–∞ —Ç–∞ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "results = results[0]  # –û—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ñ–∑ —Å–ø–∏—Å–∫—É\n",
    "annotated_image = results.plot()  # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ\n",
    "\n",
    "# –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Matplotlib\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
